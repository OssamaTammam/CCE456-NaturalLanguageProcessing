{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "SS9rq7wh9-JX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bb1845b-9131-4938-9852-4fa10980bb35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression as SKLogisticRegression"
      ],
      "metadata": {
        "id": "SGd6GunRGpCx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing and pre-processing the data"
      ],
      "metadata": {
        "id": "40OhlxEDHPfZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6gCTHbnq9-JZ"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"sst\", \"default\")\n",
        "trainDataset = dataset[\"train\"]\n",
        "validationDataset = dataset[\"validation\"]\n",
        "testDataset = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create pandas dataframes\n",
        "trainDf = dataset[\"train\"].to_pandas()\n",
        "testDf = dataset[\"test\"].to_pandas()\n",
        "validationDf = dataset[\"validation\"].to_pandas()\n",
        "dataframes = [trainDf, testDf, validationDf]"
      ],
      "metadata": {
        "id": "FGgt-kXTRWc8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assignSentimentClass(score):\n",
        "  if 0 <= score <= 0.2:\n",
        "    return 0\n",
        "  elif 0.2 < score <= 0.4:\n",
        "    return 1\n",
        "  elif 0.4 < score <= 0.6:\n",
        "    return 2\n",
        "  elif 0.6 < score <= 0.8:\n",
        "    return 3\n",
        "  else:\n",
        "    return 4"
      ],
      "metadata": {
        "id": "lOTmsCd_LgV-"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(tokens):\n",
        "  splitArray = tokens.split(\"|\")\n",
        "  return np.array(splitArray)\n",
        "\n",
        "tokenize = np.vectorize(tokenize, otypes=[object])"
      ],
      "metadata": {
        "id": "RI2qcgOKw4J0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffleDataframe(dataframe):\n",
        "  dataframe = dataframe.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "RPcKE_4BcwPZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateBigrams(tokens):\n",
        "  bigrams = []\n",
        "  for i in range(len(tokens) - 1):\n",
        "    bigram = (tokens[i], tokens[i+1])\n",
        "    bigrams.append(bigram)\n",
        "  return np.unique(np.array(bigrams), axis=0)"
      ],
      "metadata": {
        "id": "_ZmENCN1FCQ2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataframe in dataframes:\n",
        "  dataframe['sentence'] = dataframe['sentence'].str.lower()\n",
        "  dataframe['tokens'] = dataframe['tokens'].str.lower()\n",
        "  dataframe['splitTokens'] = (tokenize(np.array(dataframe['tokens'])))\n",
        "  dataframe['classes'] = dataframe['label'].apply(assignSentimentClass)\n",
        "  dataframe['bigrams'] = dataframe['splitTokens'].apply(generateBigrams)"
      ],
      "metadata": {
        "id": "_P3QA80nZWEu"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataframe in dataframes:\n",
        "  shuffleDataframe(dataframe)\n",
        "\n",
        "trainSentences = np.array(trainDf['sentence'])\n",
        "trainTokens = np.array(trainDf['splitTokens'])\n",
        "trainClasses = np.array(trainDf['classes'])\n",
        "trainLabels = np.array(trainDf['label'])\n",
        "trainBigrams = np.array(trainDf['bigrams'])\n",
        "trainFlatBigrams= np.unique(np.concatenate(trainDf['bigrams']), axis=0)\n",
        "\n",
        "testSentences = np.array(testDf['sentence'])\n",
        "testTokens = np.array(testDf['splitTokens'])\n",
        "testClasses = np.array(testDf['classes'])\n",
        "testLabels = np.array(testDf['label'])\n",
        "testBigrams = np.array(testDf['bigrams'])\n",
        "testFlatBigrams= np.unique(np.concatenate(testDf['bigrams']), axis=0)\n",
        "\n",
        "validationSentences = np.array(validationDf['sentence'])\n",
        "validationTokens = np.array(validationDf['splitTokens'])\n",
        "validationClasses = np.array(validationDf['classes'])\n",
        "validationLabels = np.array(validationDf['label'])\n",
        "validationBigrams = np.array(validationDf['bigrams'])\n",
        "testFlatBigrams= np.unique(np.concatenate(testDf['bigrams']), axis=0)"
      ],
      "metadata": {
        "id": "qCIfrd8OR6XA"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes"
      ],
      "metadata": {
        "id": "CcQKeGUpzaCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayes:\n",
        "  def __init__(self, smoothingFactor = 1):\n",
        "    self.classes = None\n",
        "    self.classesLogPrior = None\n",
        "    self.featureLogLikelihood = {0:{},1:{},2:{},3:{},4:{}}\n",
        "    self.smoothingFactor = smoothingFactor\n",
        "    self.vocab = None\n",
        "    self.bigDocs = {}\n",
        "\n",
        "  def fit(self, features, classes):\n",
        "    self.vocab = np.unique(np.concatenate(features))\n",
        "    self.classes = np.unique(classes)\n",
        "    self.calculateClassesPrior(classes)\n",
        "    self.createBigDoc(features, classes)\n",
        "    self.calculateLogLikelihood(features, classes)\n",
        "\n",
        "  def calculateClassesPrior(self, classes):\n",
        "    classFreq = np.bincount(classes)\n",
        "    totalDocs = len(classes)\n",
        "    self.classesLogPrior = np.log(classFreq / totalDocs)\n",
        "\n",
        "  def createBigDoc(self, features, classes):\n",
        "    for c in self.classes:\n",
        "      filteredArray = features[classes == c]\n",
        "      self.bigDocs[c] = np.concatenate(filteredArray)\n",
        "\n",
        "  def calculateLogLikelihood(self, features, classes):\n",
        "    for c in self.classes:\n",
        "      classDoc = self.bigDocs[c]\n",
        "      noWords = len(classDoc)\n",
        "      classVocab = np.unique(classDoc)\n",
        "      noUniqueWords = len(self.vocab)\n",
        "      for w in self.vocab:\n",
        "        count = np.count_nonzero(classDoc == w)\n",
        "        logLikelihood = np.log((count + self.smoothingFactor)/(noWords + (self.smoothingFactor * noUniqueWords)))\n",
        "        self.featureLogLikelihood[c][w] = logLikelihood\n",
        "\n",
        "  def predict(self, testDoc):\n",
        "    sum = [0 for _ in range(len(self.classes))]\n",
        "    for c in self.classes:\n",
        "      sum[c] = self.classesLogPrior[c]\n",
        "      for w in testDoc:\n",
        "        if w in self.vocab:\n",
        "          sum[c] += self.featureLogLikelihood[c][w]\n",
        "\n",
        "    return np.argmax(sum)\n",
        "\n",
        "  def testModel(self, features):\n",
        "    predictions = []\n",
        "    for i in range(len(features)):\n",
        "      prediction = self.predict(features[i])\n",
        "      predictions.append(prediction)\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "Gstu2vs7yVCD"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naiveBayes = NaiveBayes()\n",
        "naiveBayes.fit(trainTokens, trainClasses)"
      ],
      "metadata": {
        "id": "MIgXprkBlKh2"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictionNaiveBayes = naiveBayes.testModel(testTokens)"
      ],
      "metadata": {
        "id": "oV-C3U1ysRFE"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### scikit"
      ],
      "metadata": {
        "id": "Xl8TeOtMvN6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
        "scikitX = vectorizer.fit_transform(trainSentences).astype(int)\n",
        "scikitY = trainClasses\n",
        "\n",
        "scikitNaiveBayes = MultinomialNB()\n",
        "scikitNaiveBayes.fit(scikitX, scikitY)\n",
        "scikitNaiveBayesPredictions = scikitNaiveBayes.predict(vectorizer.transform(testSentences).astype(int))"
      ],
      "metadata": {
        "id": "WcNjSSwxvS-S"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluations"
      ],
      "metadata": {
        "id": "GIsD-B88yFPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def confusionMatrix(predictions, realValues, numClasses):\n",
        "  confMatrix = np.zeros((numClasses, numClasses), dtype=int)\n",
        "\n",
        "  for pred, value in zip(predictions, realValues):\n",
        "    confMatrix[pred, value] += 1\n",
        "\n",
        "  return confMatrix\n",
        "\n",
        "def computeMetrics(confMatrix):\n",
        "    noClasses = confMatrix.shape[0]\n",
        "    precision = np.zeros(noClasses)\n",
        "    recall = np.zeros(noClasses)\n",
        "    f1 = np.zeros(noClasses)\n",
        "\n",
        "    for i in range(noClasses):\n",
        "        truePositive = confMatrix[i, i]\n",
        "        falsePositive = np.sum(confMatrix[i, :]) - truePositive\n",
        "        falseNegative = np.sum(confMatrix[:, i]) - truePositive\n",
        "\n",
        "        precision[i] = truePositive / (truePositive + falsePositive) if (truePositive + falsePositive) != 0 else 0\n",
        "        recall[i] = truePositive / (truePositive + falseNegative) if (truePositive + falseNegative) != 0 else 0\n",
        "\n",
        "        f1[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i]) if (precision[i] + recall[i]) != 0 else 0\n",
        "\n",
        "    # Macro-averaged precision, recall, and F1 score\n",
        "    macroPrecision = np.mean(precision)\n",
        "    macroRecall = np.mean(recall)\n",
        "    macroF1 = np.mean(f1)\n",
        "\n",
        "    return precision, recall, f1, macroPrecision, macroRecall, macroF1\n",
        "\n",
        "def printMetrics(metricsTuple):\n",
        "  precision, recall, f1, macroPrecision, macroRecall, macroF1 = metricsTuple\n",
        "  print(\"Precision: \", precision)\n",
        "  print(\"Recall: \", recall)\n",
        "  print(\"F1: \", f1)\n",
        "  print(\"Macro Precision: \", macroPrecision)\n",
        "  print(\"Macro Recall: \", macroRecall)\n",
        "  print(\"Macro F1: \", macroF1)"
      ],
      "metadata": {
        "id": "8wOJDYklyHCG"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Naive Bayes"
      ],
      "metadata": {
        "id": "b_xmZYuL0JyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Our own Naive Bayes model"
      ],
      "metadata": {
        "id": "XISNOfiiWHLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naiveBayesConfMatrix = confusionMatrix(predictionNaiveBayes, testClasses, len(np.unique(testClasses)))\n",
        "metrics = computeMetrics(naiveBayesConfMatrix)\n",
        "printMetrics(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNB1yYdi0MTs",
        "outputId": "76b372d0-0929-4c20-c5ce-89f8640153a0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision:  [0.4        0.45394737 0.21848739 0.3527668  0.58267717]\n",
            "Recall:  [0.05734767 0.65402844 0.06683805 0.7        0.18546366]\n",
            "F1:  [0.10031348 0.53592233 0.1023622  0.46911958 0.28136882]\n",
            "Macro Precision:  0.4015757454304678\n",
            "Macro Recall:  0.33273556233804336\n",
            "Macro F1:  0.2978172830477508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scikit Naive Bayes model"
      ],
      "metadata": {
        "id": "K-Exv030WJij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scikitNaiveBayesConfMatrix = confusionMatrix(scikitNaiveBayesPredictions, testClasses, len(np.unique(testClasses)))\n",
        "printMetrics(computeMetrics(scikitNaiveBayesConfMatrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niTpEH5GUvpK",
        "outputId": "d7c229e2-3ce9-41ba-a39c-3d4dd108cab8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision:  [0.38202247 0.40586797 0.19487179 0.3219107  0.54482759]\n",
            "Recall:  [0.1218638  0.52448657 0.09768638 0.60784314 0.19799499]\n",
            "F1:  [0.18478261 0.45761544 0.13013699 0.42090971 0.29044118]\n",
            "Macro Precision:  0.36990010387828437\n",
            "Macro Recall:  0.30997497424160025\n",
            "Macro F1:  0.2967771834351165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression\n"
      ],
      "metadata": {
        "id": "Mt3VyC8hXB0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
        "trainFeatures = vectorizer.fit_transform(trainSentences).astype(int)\n",
        "testFeatures = vectorizer.transform(testSentences).astype(int)\n",
        "validationFeatures = vectorizer.transform(validationSentences).astype(int)"
      ],
      "metadata": {
        "id": "0vxQriXMKZt6"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, learningRate=0.01, noIterations=100):\n",
        "        self.learningRate = learningRate\n",
        "        self.noIterations = noIterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def initialize_parameters(self, noFeatures):\n",
        "        self.weights = np.zeros(noFeatures)\n",
        "        self.bias = 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        noSamples, noFeatures = X.shape\n",
        "        self.initialize_parameters(noFeatures)\n",
        "\n",
        "        for _ in range(self.noIterations):\n",
        "            # Shuffle the data\n",
        "            shuffled_indices = np.random.permutation(noSamples)\n",
        "            X_shuffled = X[shuffled_indices]\n",
        "            y_shuffled = y[shuffled_indices]\n",
        "\n",
        "            linearModel = X_shuffled.dot(self.weights.T) + self.bias\n",
        "            yPred = self.sigmoid(linearModel)\n",
        "\n",
        "            dw = (1 / noSamples) * X_shuffled.T.dot((yPred - y_shuffled))\n",
        "            db = (1 / noSamples) * np.sum(yPred - y_shuffled)\n",
        "\n",
        "            self.weights -= self.learningRate * dw\n",
        "            self.bias -= self.learningRate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linearModel = X.dot(self.weights.T) + self.bias\n",
        "        yPred = self.sigmoid(linearModel)\n",
        "        yPredClass = np.where(yPred > 0.5, 1, 0)\n",
        "        return yPredClass\n"
      ],
      "metadata": {
        "id": "ckM2E279Kq0f"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logisticRegresion = LogisticRegression()\n",
        "logisticRegresion.fit(trainFeatures, trainClasses)"
      ],
      "metadata": {
        "id": "Gr-JXW-ZKyLr"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logisticRegresionPredictions = logisticRegresion.predict(testFeatures)\n",
        "print(f\" Accuracy : {accuracy_score(testClasses, logisticRegresionPredictions)*100}\")"
      ],
      "metadata": {
        "id": "7etPbanpmH90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9709ce2-71d4-4a78-ccd8-d12fb1224c68"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy : 28.64253393665158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lrConfusionMatrix = confusionMatrix(logisticRegresionPredictions, testClasses, len(np.unique(testClasses)))\n",
        "printMetrics(computeMetrics(lrConfusionMatrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avqK2AGVUhCH",
        "outputId": "001b3f84-71b0-4eee-959b-5cc2d301c429"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision:  [0.         0.28642534 0.         0.         0.        ]\n",
            "Recall:  [0. 1. 0. 0. 0.]\n",
            "F1:  [0.         0.44530426 0.         0.         0.        ]\n",
            "Macro Precision:  0.05728506787330316\n",
            "Macro Recall:  0.2\n",
            "Macro F1:  0.08906085121350686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit"
      ],
      "metadata": {
        "id": "_YeBLym7WSSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scitkitLogisticRegression = SKLogisticRegression()\n",
        "scitkitLogisticRegression.fit(trainFeatures, trainClasses)\n",
        "scitkitLogisticRegressionPredictions = scitkitLogisticRegression.predict(testFeatures)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgkSFUtxWTUY",
        "outputId": "fe0c333c-2f31-43ad-bbee-d1b4c45aa807"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scitkitLogisticRegressionConfMatrix = confusionMatrix(scitkitLogisticRegressionPredictions, testClasses, len(np.unique(testClasses)))\n",
        "printMetrics(computeMetrics(scitkitLogisticRegressionConfMatrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylik8P7NWrq7",
        "outputId": "b24d64d9-ac68-4239-b4eb-3bc83148c67c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision:  [0.3364486  0.39498807 0.20430108 0.33660934 0.48837209]\n",
            "Recall:  [0.12903226 0.52290679 0.14652956 0.5372549  0.21052632]\n",
            "F1:  [0.1865285  0.45003399 0.17065868 0.41389728 0.29422067]\n",
            "Macro Precision:  0.3521438339716053\n",
            "Macro Recall:  0.3092499663691505\n",
            "Macro F1:  0.3030678233985227\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}