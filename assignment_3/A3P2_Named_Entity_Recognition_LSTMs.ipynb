{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6czvz5VKO5M"
      },
      "source": [
        "# Notebook for Programming in Problem 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o8HI5JqTvU5"
      },
      "source": [
        "## Learning Objectives\n",
        "In this problem, we will use [PyTorch](https://pytorch.org/) to implement long short-term memory (LSTM) for named entity recognition (NER). We will use the same dataset and boilerplate code as in Programming Problem 1 of Assignment #3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObrHyvWvTyGZ"
      },
      "source": [
        "## Writing Code\n",
        "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
        "Feel free to change function signatures, but be careful that you might need to also change how they are called in other parts of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r6YTnpgbFdMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68810678-d917-4fee-fd1f-1e629a80895d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 30 11:02:36 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi # you may need to try reconnecting to get a T4 gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnYMKJlKNXYe"
      },
      "source": [
        "## Installing PyTorch and Other Packages\n",
        "\n",
        "Install PyTorch using pip. See [https://pytorch.org/](https://pytorch.org/) if you want to install it on your computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-dRVuiP_JVdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f1e403-7339-467f-85f9-46bedec4ecca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchtext -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPsFH637OpLy"
      },
      "source": [
        "Test if our installation works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c62StNb2NvKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd3e4894-a268-42b5-adcb-eb97224cf132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch successfully installed!\n",
            "Version: 2.2.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Multiply two matrices on GPU\n",
        "a = torch.rand(100, 200).cuda()\n",
        "b = torch.rand(200, 100).cuda()\n",
        "c = torch.matmul(a, b)\n",
        "\n",
        "print(\"PyTorch successfully installed!\")\n",
        "print(\"Version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qaC8sxcqkGX"
      },
      "source": [
        "Also install [scikit-learn](https://scikit-learn.org/stable/). We will use it for calculating evaluation metrics such as accuracy and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i5Y2xB_uqqM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d28d63d4-aade-4e22-e5e5-9a083e1713c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhV4CYivRbt4"
      },
      "source": [
        "Let's import all the packages at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EjRM4cCFRh-d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import Vocab, vocab\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict, Optional, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn1bIPjAN-9V"
      },
      "source": [
        "## Long Short Term Memory (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJOKIneRTrTH"
      },
      "source": [
        "### Data Loading\n",
        "\n",
        "We will use the same dataset for named entity recognition in Assignment #2. First download the data and take a look at the first 50 lines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lWqz7kDxSqeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6687f818-e707-487a-c562-294d6bb02f6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EU NNP I-NP ORG\n",
            "rejects VBZ I-VP O\n",
            "German JJ I-NP MISC\n",
            "call NN I-NP O\n",
            "to TO I-VP O\n",
            "boycott VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Peter NNP I-NP PER\n",
            "Blackburn NNP I-NP PER\n",
            "\n",
            "BRUSSELS NNP I-NP LOC\n",
            "1996-08-22 CD I-NP O\n",
            "\n",
            "The DT I-NP O\n",
            "European NNP I-NP ORG\n",
            "Commission NNP I-NP ORG\n",
            "said VBD I-VP O\n",
            "on IN I-PP O\n",
            "Thursday NNP I-NP O\n",
            "it PRP B-NP O\n",
            "disagreed VBD I-VP O\n",
            "with IN I-PP O\n",
            "German JJ I-NP MISC\n",
            "advice NN I-NP O\n",
            "to TO I-PP O\n",
            "consumers NNS I-NP O\n",
            "to TO I-VP O\n",
            "shun VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            "until IN I-SBAR O\n",
            "scientists NNS I-NP O\n",
            "determine VBP I-VP O\n",
            "whether IN I-SBAR O\n",
            "mad JJ I-NP O\n",
            "cow NN I-NP O\n",
            "disease NN I-NP O\n",
            "can MD I-VP O\n",
            "be VB I-VP O\n",
            "transmitted VBN I-VP O\n",
            "to TO I-PP O\n",
            "sheep NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Germany NNP I-NP LOC\n",
            "'s POS B-NP O\n",
            "representative NN I-NP O\n"
          ]
        }
      ],
      "source": [
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
        "!cat eng.train | head -n 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVt1a6nzWsiF"
      },
      "source": [
        "Each line corresponds to a word. Different sentences are separated by an additional line break. Take \"EU NNP I-NP ORG\" as an example. \"EU\" is a word. \"NNP\" and \"I-NP\" are tags for POS tagging and chunking, which we will ignore. \"ORG\" is the tag for NER, which is our prediction target. There are 5 possible values for the NER tag: ORG, PER, LOC, MISC, and O.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WnNfOBUYJvVW"
      },
      "outputs": [],
      "source": [
        "# A sentence is a list of (word, tag) tuples.\n",
        "# For example, [(\"hello\", \"O\"), (\"world\", \"O\"), (\"!\", \"O\")]\n",
        "Sentence = List[Tuple[str, str]]\n",
        "\n",
        "\n",
        "def read_data_file(\n",
        "    datapath: str,\n",
        ") -> Tuple[List[Sentence], Dict[str, int], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Read and preprocess input data from the file `datapath`.\n",
        "    Example:\n",
        "    ```\n",
        "        sentences, word_cnt, tag_cnt = read_data_file(\"eng.train\")\n",
        "    ```\n",
        "    Return values:\n",
        "        `sentences`: a list of sentences, including words and NER tags\n",
        "        `word_cnt`: a Counter object, the number of occurrences of each word\n",
        "        `tag_cnt`: a Counter object, the number of occurences of each NER tag\n",
        "    \"\"\"\n",
        "    sentences: List[Sentence] = []\n",
        "    word_cnt: Dict[str, int] = Counter()\n",
        "    tag_cnt: Dict[str, int] = Counter()\n",
        "\n",
        "    for sentence_txt in open(datapath).read().split(\"\\n\\n\"):\n",
        "        if \"DOCSTART\" in sentence_txt:\n",
        "            # Ignore dummy sentences at the begining of each document.\n",
        "            continue\n",
        "        # Read a new sentence\n",
        "        sentences.append([])\n",
        "        for token in sentence_txt.split(\"\\n\"):\n",
        "            w, _, _, t = token.split()\n",
        "            # Replace all digits with \"0\" to reduce out-of-vocabulary words\n",
        "            w = re.sub(\"\\d\", \"0\", w)\n",
        "            word_cnt[w] += 1\n",
        "            tag_cnt[t] += 1\n",
        "            sentences[-1].append((w, t))\n",
        "\n",
        "    return sentences, word_cnt, tag_cnt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WLMGYSZ7KxzP"
      },
      "outputs": [],
      "source": [
        "# Some helper code\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"\n",
        "    Use GPU when it is available; use CPU otherwise.\n",
        "    See https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code\n",
        "    \"\"\"\n",
        "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wVHAOb7iMPwC"
      },
      "outputs": [],
      "source": [
        "def eval_metrics(ground_truth: List[int], predictions: List[int]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate various evaluation metrics such as accuracy and F1 score\n",
        "    Parameters:\n",
        "        `ground_truth`: the list of ground truth NER tags\n",
        "        `predictions`: the list of predicted NER tags\n",
        "    \"\"\"\n",
        "    f1_scores = f1_score(ground_truth, predictions, average=None)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(ground_truth, predictions),\n",
        "        \"f1\": f1_scores,\n",
        "        \"average f1\": np.mean(f1_scores),\n",
        "        \"confusion matrix\": confusion_matrix(ground_truth, predictions),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s830dhbnj1L"
      },
      "source": [
        "## Long Short-term Memory (LSTM)\n",
        "\n",
        "Now we implement an one-layer LSTM for the same task and compare it to FFNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to7DnWNiY5ZS"
      },
      "source": [
        "### Data Loading **(4 points)**\n",
        "\n",
        "Like before, we first implement the data loader. But unlike before, each data example is now a variable-length sentence. How can we pack multiple sentences with different lengths into the same batch? One possible solution is to pad them to the same length using a special token. The code below illustrates the idea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J5oVgqE7JaJp"
      },
      "outputs": [],
      "source": [
        "# 3 sentences with different lengths\n",
        "sentence_1 = torch.tensor([6, 1, 2])\n",
        "sentence_2 = torch.tensor([4, 2, 7, 7, 9])\n",
        "sentence_3 = torch.tensor([3, 4])\n",
        "# Form a batch by padding 0\n",
        "sentence_batch = torch.tensor([\n",
        "    [6, 1, 2, 0, 0],\n",
        "    [4, 2, 7, 7, 9],\n",
        "    [3, 4, 0, 0, 0],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udC0SMjkKaCN"
      },
      "source": [
        "We implement the above idea in a customized batching function `form_batch`. Optionally, see [here](https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data) for how batching works in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sACcGN4XYMgj"
      },
      "outputs": [],
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each data example is a sentence, including its words and NER tags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, datapath: str, words_vocab: Optional[Vocab] = None, tags_vocab: Optional[Vocab] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the dataset by reading from datapath.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.sentences: List[Sentence] = []\n",
        "        UNKNOWN = \"<UNKNOWN>\"\n",
        "        PAD = \"<PAD>\"  # Special token used for padding\n",
        "\n",
        "        print(\"Loading data from %s\" % datapath)\n",
        "        self.sentences, word_cnt, tag_cnt = read_data_file(datapath)\n",
        "        print(\"%d sentences loaded.\" % len(self.sentences))\n",
        "\n",
        "        if words_vocab is None:\n",
        "            words_vocab = vocab(word_cnt, specials=[PAD, UNKNOWN])\n",
        "            words_vocab.set_default_index(words_vocab[UNKNOWN])\n",
        "\n",
        "        self.words_vocab = words_vocab\n",
        "\n",
        "        self.unknown_idx = self.words_vocab[UNKNOWN]\n",
        "        self.pad_idx = self.words_vocab[PAD]\n",
        "\n",
        "        if tags_vocab is None:\n",
        "            tags_vocab = vocab(tag_cnt, specials=[])\n",
        "        self.tags_vocab = tags_vocab\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Sentence:\n",
        "        \"\"\"\n",
        "        Get the idx'th sentence in the dataset.\n",
        "        \"\"\"\n",
        "        return self.sentences[idx]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the number of sentences in the dataset.\n",
        "        \"\"\"\n",
        "        # TODO: Implement this method\n",
        "        # START HERE\n",
        "        return len(self.sentences)\n",
        "        # END\n",
        "\n",
        "    def form_batch(self, sentences: List[Sentence]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        A customized function for batching a number of sentences together.\n",
        "        Different sentences have different lengths. Let max_len be the longest length.\n",
        "        When packing them into one tensor, we need to pad all sentences to max_len.\n",
        "        Return values:\n",
        "            `words`: a list in which each element itself is a list of words in a sentence\n",
        "            `word_idxs`: a batch_size x max_len tensor.\n",
        "                       word_idxs[i][j] is the index of the j'th word in the i'th sentence .\n",
        "            `tags`: a list in which each element itself is a list of tags in a sentence\n",
        "            `tag_idxs`: a batch_size x max_len tensor\n",
        "                      tag_idxs[i][j] is the index of the j'th tag in the i'th sentence.\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "                        valid_mask[i][j] is True if the i'th sentence has the j'th word.\n",
        "                        Otherwise, valid[i][j] is False.\n",
        "        \"\"\"\n",
        "        words: List[List[str]] = []\n",
        "        tags: List[List[str]] = []\n",
        "        max_len = -1  # length of the longest sentence\n",
        "        for sent in sentences:\n",
        "            words.append([])\n",
        "            tags.append([])\n",
        "            for w, t in sent:\n",
        "                words[-1].append(w)\n",
        "                tags[-1].append(t)\n",
        "            max_len = max(max_len, len(words[-1]))\n",
        "\n",
        "        batch_size = len(sentences)\n",
        "        word_idxs = torch.full(\n",
        "            (batch_size, max_len), fill_value=self.pad_idx, dtype=torch.int64\n",
        "        )\n",
        "        tag_idxs = torch.full_like(word_idxs, fill_value=self.tags_vocab[\"O\"])\n",
        "        valid_mask = torch.zeros_like(word_idxs, dtype=torch.bool)\n",
        "\n",
        "        ## TODO: Fill in the values in word_idxs, tag_idxs, and valid_mask\n",
        "        ## Caveat: There may be out-of-vocabulary words in validation data\n",
        "        ## See torchtext.vocab.Vocab: https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab\n",
        "        ## START HERE\n",
        "\n",
        "        for i, sent in enumerate(sentences):\n",
        "          for j, (w, t) in enumerate(sent):\n",
        "              word_idxs[i][j] = self.words_vocab[w]\n",
        "              tag_idxs[i][j] = self.tags_vocab[t]\n",
        "              valid_mask[i][j] = True\n",
        "\n",
        "        # END\n",
        "\n",
        "        return {\n",
        "            \"words\": words,\n",
        "            \"word_idxs\": word_idxs,\n",
        "            \"tags\": tags,\n",
        "            \"tag_idxs\": tag_idxs,\n",
        "            \"valid_mask\": valid_mask,\n",
        "        }\n",
        "\n",
        "\n",
        "def create_sequence_dataloaders(\n",
        "    batch_size: int, shuffle: bool = True\n",
        ") -> Tuple[DataLoader, DataLoader, Vocab]:\n",
        "    \"\"\"\n",
        "    Create the dataloaders for training and validaiton.\n",
        "    \"\"\"\n",
        "    ds_train = SequenceDataset(\"eng.train\")\n",
        "    ds_val = SequenceDataset(\"eng.val\", words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n",
        "    loader_train = DataLoader(\n",
        "        ds_train,\n",
        "        batch_size,\n",
        "        shuffle,\n",
        "        collate_fn=ds_train.form_batch,  # customized function for batching\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    loader_val = DataLoader(\n",
        "        ds_val, batch_size, collate_fn=ds_val.form_batch, pin_memory=True\n",
        "    )\n",
        "    return loader_train, loader_val, ds_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2EcVxYuYvGv"
      },
      "source": [
        "Here is a simple sanity-check. Try to understand its output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TazmodGWYx2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fc629c-1192-4940-fc18-ac08c2298384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "Iterating on the training data..\n",
            "{'words': [['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '0000-00-00']], 'word_idxs': tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
            "        [11, 12,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [13, 14,  0,  0,  0,  0,  0,  0,  0]]), 'tags': [['ORG', 'O', 'MISC', 'O', 'O', 'O', 'MISC', 'O', 'O'], ['PER', 'PER'], ['LOC', 'O']], 'tag_idxs': tensor([[0, 1, 2, 1, 1, 1, 2, 1, 1],\n",
            "        [3, 3, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [4, 1, 1, 1, 1, 1, 1, 1, 1]]), 'valid_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
            "        [ True,  True, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False, False, False, False, False, False, False]])}\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "def check_sequence_dataloader() -> None:\n",
        "    loader_train, _, _ = create_sequence_dataloaders(batch_size=3, shuffle=False)\n",
        "    print(\"Iterating on the training data..\")\n",
        "    for i, data_batch in enumerate(loader_train):\n",
        "        if i == 0:\n",
        "            print(data_batch)\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "check_sequence_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifk3i-obY8YB"
      },
      "source": [
        "### Implement the Model **(8 points)**\n",
        "\n",
        "Next, implement LSTM for predicting NER tags from input words. [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) is definitely useful. Further, it is tricky to handle sentences in the same batch with different lengths. Please read the PyTorch documentation in detail!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3V0NvQynZF8e"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Long short-term memory for NER\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, words_vocab: Vocab, tags_vocab:Vocab, d_emb: int, d_hidden: int, bidirectional: bool,pad_idx: int) -> None:\n",
        "        \"\"\"\n",
        "        Initialize an LSTM\n",
        "        Parameters:\n",
        "            `words_vocab`: vocabulary of words\n",
        "            `tags_vocab`: vocabulary of tags\n",
        "            `d_emb`: dimension of word embeddings (D)\n",
        "            `d_hidden`: dimension of the hidden layer (H)\n",
        "            `bidirectional`: true if LSTM should be bidirectional\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: Create the word embeddings (nn.Embedding),\n",
        "        #       the LSTM (nn.LSTM) and the output layer (nn.Linear).\n",
        "        #       Read the torch docs for additional guidance : https://pytorch.org/docs/stable\n",
        "        #       Note: Pay attention to the LSTM output shapes!\n",
        "        # START HERE\n",
        "        self.words_vocab = words_vocab\n",
        "        self.tags_vocab = tags_vocab\n",
        "        self.d_hidden = d_hidden\n",
        "        self.bidirectional = bidirectional\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "        # Create word embeddings\n",
        "        self.embedding = nn.Embedding(len(words_vocab), d_emb)\n",
        "\n",
        "        # Create LSTM layer\n",
        "        self.lstm = nn.LSTM(d_emb, d_hidden, bidirectional=bidirectional, batch_first=True)\n",
        "\n",
        "        # Create output layer\n",
        "        self.output_layer = nn.Linear(d_hidden * 2 if bidirectional else d_hidden, len(tags_vocab))\n",
        "\n",
        "        # END\n",
        "\n",
        "    def forward(\n",
        "        self, word_idxs: torch.Tensor, valid_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Given words in sentences, predict the logits of the NER tag.\n",
        "        Parameters:\n",
        "            `word_idxs`: a batch_size x max_len tensor\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "        Return values:\n",
        "            `logits`: a batch_size x max_len x 5 tensor\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass\n",
        "        # START HERE\n",
        "        # Apply word embeddings\n",
        "        word_embeddings = self.embedding(word_idxs)  # batch_size x max_len x d_emb\n",
        "\n",
        "        # Mask out paddings\n",
        "        word_embeddings = word_embeddings * valid_mask.unsqueeze(-1)\n",
        "\n",
        "        # LSTM layer\n",
        "        lstm_output, _ = self.lstm(word_embeddings)\n",
        "\n",
        "        # Output layer\n",
        "        logits = self.output_layer(lstm_output)\n",
        "        # END\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BFTKaB4Zydx"
      },
      "source": [
        "We do a sanity-check by loading a batch of data examples and pass it through the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PKg1ni4QZ6D1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c70e000-7841-446f-fe42-7cdd27debc97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (words_vocab): Vocab()\n",
            "  (tags_vocab): Vocab()\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
            "  (output_layer): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Input word_idxs shape: torch.Size([4, 11])\n",
            "Input valid_mask shape: torch.Size([4, 11])\n",
            "Output logits shape: torch.Size([4, 11, 5])\n"
          ]
        }
      ],
      "source": [
        "def check_lstm() -> None:\n",
        "    # Hyperparameters\n",
        "    batch_size = 4\n",
        "    d_emb = 64\n",
        "    d_hidden = 128\n",
        "    bidirectional = True\n",
        "    # Create the dataloaders and the model\n",
        "    loader_train, _, ds_train = create_sequence_dataloaders(batch_size)\n",
        "    model = LSTM(ds_train.words_vocab, ds_train.tags_vocab, d_emb, d_hidden, bidirectional,ds_train.pad_idx)\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Get the first batch\n",
        "    data_batch = next(iter(loader_train))\n",
        "    # Move data to GPU\n",
        "    word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "    tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "    valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "    # Calculate the model\n",
        "    print(\"Input word_idxs shape:\", word_idxs.size())\n",
        "    print(\"Input valid_mask shape:\", valid_mask.size())\n",
        "    logits = model(word_idxs, valid_mask)\n",
        "    print(\"Output logits shape:\", logits.size())\n",
        "\n",
        "\n",
        "check_lstm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jddDYUiLY-hc"
      },
      "source": [
        "### Training and Validation **(6 points)**\n",
        "\n",
        "Complete the functions for training and validating the LSTM model. When calculating the loss function, you only want to include values from valid positions (where `valid_mask` is `True`). The `reduction` parameter in [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy) may be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hv_15mnXZ_dy"
      },
      "outputs": [],
      "source": [
        "def train_lstm(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    silent: bool = False,  # whether to print the training loss\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Train the LSTM model.\n",
        "    Return values:\n",
        "        1. the average training loss\n",
        "        2. training metrics such as accuracy and F1 score\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "    report_interval = 100\n",
        "\n",
        "    for i, data_batch in enumerate(loader):\n",
        "        word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "        tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "        valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "        # TODO: Do the same tasks as train_ffnn\n",
        "        # START HERE\n",
        "        # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
        "        # Compute logits\n",
        "        logits = model(word_idxs, valid_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), tag_idxs.flatten(), ignore_index=model.pad_idx)\n",
        "\n",
        "        # END\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # we get (unmasked) predictions by getting argmax of logits along last dimension (You will need to define logits!)\n",
        "        net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "        # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
        "        tag_idxs_flat = tag_idxs.flatten()\n",
        "        valid_mask_flat = valid_mask.flatten()\n",
        "        net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "        ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "        predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "        if not silent and i > 0 and i % report_interval == 0:\n",
        "            print(\n",
        "                \"\\t[%06d/%06d] Loss: %f\"\n",
        "                % (i, len(loader), np.mean(losses[-report_interval:]))\n",
        "            )\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def validate_lstm(\n",
        "    model: nn.Module, loader: DataLoader, device: torch.device\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Validate the model.\n",
        "    Return the validation loss and metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for data_batch in loader:\n",
        "            word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "            tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "            valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "            # TODO: Do the same tasks as validate_ffnn\n",
        "            # START HERE\n",
        "            # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
        "            logits = model(word_idxs, valid_mask)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), tag_idxs.flatten(), ignore_index=model.pad_idx)\n",
        "            # END\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # we get (unmasked) predictions by getting argmax of logits (You will need to define logits!)\n",
        "            net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "            # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
        "            tag_idxs_flat = tag_idxs.flatten()\n",
        "            valid_mask_flat = valid_mask.flatten()\n",
        "            net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "            ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "            predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def train_val_loop_lstm(hyperparams: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Train and validate the LSTM model for a number of epochs.\n",
        "    \"\"\"\n",
        "    print(\"Hyperparameters:\", hyperparams)\n",
        "    # Create the dataloaders\n",
        "    loader_train, loader_val, ds_train = create_sequence_dataloaders(\n",
        "        hyperparams[\"batch_size\"]\n",
        "    )\n",
        "    # Create the model\n",
        "    model = LSTM(\n",
        "        ds_train.words_vocab,\n",
        "        ds_train.tags_vocab,\n",
        "        hyperparams[\"d_emb\"],\n",
        "        hyperparams[\"d_hidden\"],\n",
        "        hyperparams[\"bidirectional\"],\n",
        "        ds_train.pad_idx\n",
        "    )\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Create the optimizer\n",
        "    optimizer = optim.RMSprop(\n",
        "        model.parameters(), hyperparams[\"learning_rate\"], weight_decay=hyperparams[\"l2\"]\n",
        "    )\n",
        "\n",
        "    # Train and validate\n",
        "    for i in range(hyperparams[\"num_epochs\"]):\n",
        "        print(\"Epoch #%d\" % i)\n",
        "\n",
        "        print(\"Training..\")\n",
        "        loss_train, metrics_train = train_lstm(model, loader_train, optimizer, device)\n",
        "        print(\"Training loss: \", loss_train)\n",
        "        print(\"Training metrics:\")\n",
        "        for k, v in metrics_train.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "        print(\"Validating..\")\n",
        "        loss_val, metrics_val = validate_lstm(model, loader_val, device)\n",
        "        print(\"Validation loss: \", loss_val)\n",
        "        print(\"Validation metrics:\")\n",
        "        for k, v in metrics_val.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU9Nef7yal_M"
      },
      "source": [
        "Run the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "pFxQxlokai6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32b1649-aa24-4f23-e6c8-087c82b20f75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': True, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (words_vocab): Vocab()\n",
            "  (tags_vocab): Vocab()\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
            "  (output_layer): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.24692961725371856\n",
            "Training metrics:\n",
            "\t accuracy :  0.8005945997176649\n",
            "\t f1 :  [0.02565756 0.89573465 0.0318942  0.19261581 0.35149782]\n",
            "\t average f1 :  0.2994800102679984\n",
            "\t confusion matrix :  [[   179   8686    176    395    450]\n",
            " [  3361 156200   2540   3682   1178]\n",
            " [   104   3321    123    245    736]\n",
            " [   234   8401    214   1659    422]\n",
            " [   189   5195    131    315   2335]]\n",
            "Validating..\n",
            "Validation loss:  0.062359925891671865\n",
            "Validation metrics:\n",
            "\t accuracy :  0.8819622703011042\n",
            "\t f1 :  [0.         0.94382564 0.24356775 0.45136387 0.62156448]\n",
            "\t average f1 :  0.45206434979032456\n",
            "\t confusion matrix :  [[    0  1881    10   115   244]\n",
            " [    0 41097     3    34    30]\n",
            " [    0   552   142    54   259]\n",
            " [    0  1711     2   877   100]\n",
            " [    0   681     2   116  1176]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.04064774395966971\n",
            "Training metrics:\n",
            "\t accuracy :  0.9040454880698467\n",
            "\t f1 :  [0.         0.96031451 0.49875312 0.70683596 0.6912935 ]\n",
            "\t average f1 :  0.5714394180233862\n",
            "\t confusion matrix :  [[     0   7356    294   1082   1157]\n",
            " [     0 166350     78    443    223]\n",
            " [     0   1321   1700    506   1010]\n",
            " [     0   2916     61   7543    463]\n",
            " [     0   1412    147    786   5820]]\n",
            "Validating..\n",
            "Validation loss:  0.0378403937710183\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9122356680112456\n",
            "\t f1 :  [0.         0.96226128 0.61639737 0.69759179 0.7494856 ]\n",
            "\t average f1 :  0.6051472054511713\n",
            "\t confusion matrix :  [[    0  1553    86   333   278]\n",
            " [    0 41039    19    75    31]\n",
            " [    0   307   515    79   106]\n",
            " [    0   870    12  1767    41]\n",
            " [    0   364    32   122  1457]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.025001573617811555\n",
            "Training metrics:\n",
            "\t accuracy :  0.9266737884626904\n",
            "\t f1 :  [0.         0.9714284  0.72091131 0.8328529  0.79415191]\n",
            "\t average f1 :  0.6638689046460435\n",
            "\t confusion matrix :  [[     0   6665    554   1312   1359]\n",
            " [     0 166327    103    247    147]\n",
            " [     0    712   3101    281    430]\n",
            " [     0   1182     99   9390    253]\n",
            " [     0    728    222    395   6817]]\n",
            "Validating..\n",
            "Validation loss:  0.0327761388782944\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9196919691969196\n",
            "\t f1 :  [0.         0.96479543 0.69675289 0.7547619  0.78393493]\n",
            "\t average f1 :  0.6400490306689417\n",
            "\t confusion matrix :  [[    0  1550   109   277   314]\n",
            " [    0 41067    30    46    21]\n",
            " [    0   271   633    48    55]\n",
            " [    0   749    12  1902    27]\n",
            " [    0   330    26    77  1542]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.012226146576857125\n",
            "Training metrics:\n",
            "\t accuracy :  0.9385799513148968\n",
            "\t f1 :  [0.         0.9769162  0.80314961 0.89774306 0.84590981]\n",
            "\t average f1 :  0.7047437350437699\n",
            "\t confusion matrix :  [[     0   6443    631   1221   1578]\n",
            " [     0 166679     79    102     69]\n",
            " [     0    405   3672    196    257]\n",
            " [     0    437     79  10342    116]\n",
            " [     0    342    153    205   7466]]\n",
            "Validating..\n",
            "Validation loss:  0.027553835351552283\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9250499123986473\n",
            "\t f1 :  [0.         0.96856162 0.72204473 0.78546976 0.802589  ]\n",
            "\t average f1 :  0.6557330211934204\n",
            "\t confusion matrix :  [[    0  1448   117   335   350]\n",
            " [    0 41052    34    56    22]\n",
            " [    0   242   678    48    39]\n",
            " [    0   595    11  2065    19]\n",
            " [    0   268    31    64  1612]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.007044772427805044\n",
            "Training metrics:\n",
            "\t accuracy :  0.9443585857578719\n",
            "\t f1 :  [0.         0.9793457  0.8546331  0.92815855 0.865488  ]\n",
            "\t average f1 :  0.7255250703225846\n",
            "\t confusion matrix :  [[     0   6369    603   1116   1755]\n",
            " [     0 166952     51     38     43]\n",
            " [     0    243   3989    118    173]\n",
            " [     0    141     55  10678     74]\n",
            " [     0    157    114    111   7808]]\n",
            "Validating..\n",
            "Validation loss:  0.026131696200796535\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9271686427902049\n",
            "\t f1 :  [0.         0.96949289 0.72908995 0.80476013 0.81207886]\n",
            "\t average f1 :  0.6630843671428523\n",
            "\t confusion matrix :  [[    0  1438   125   336   351]\n",
            " [    0 41027    41    78    18]\n",
            " [    0   238   693    52    24]\n",
            " [    0   507     7  2164    12]\n",
            " [    0   262    28    58  1627]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.0043612188590621504\n",
            "Training metrics:\n",
            "\t accuracy :  0.947330655850433\n",
            "\t f1 :  [0.         0.98004487 0.88506959 0.94570806 0.88128839]\n",
            "\t average f1 :  0.738422182488301\n",
            "\t confusion matrix :  [[     0   6454    617   1004   1779]\n",
            " [     0 167105     38     16     25]\n",
            " [     0    148   4197     65    106]\n",
            " [     0     47     36  10852     31]\n",
            " [     0     77     80     47   7962]]\n",
            "Validating..\n",
            "Validation loss:  0.026483749438609396\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9280242839098725\n",
            "\t f1 :  [0.         0.96941705 0.74443266 0.81711473 0.80995365]\n",
            "\t average f1 :  0.6681836167748135\n",
            "\t confusion matrix :  [[    0  1466   113   269   402]\n",
            " [    0 41033    37    68    26]\n",
            " [    0   234   702    47    24]\n",
            " [    0   512     8  2158    12]\n",
            " [    0   246    19    50  1660]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.0028902218694350232\n",
            "Training metrics:\n",
            "\t accuracy :  0.9486422462159938\n",
            "\t f1 :  [0.         0.98001338 0.90697918 0.95733345 0.8871874 ]\n",
            "\t average f1 :  0.7463026816286008\n",
            "\t confusion matrix :  [[     0   6573    616    849   1842]\n",
            " [     0 166959     35     12     20]\n",
            " [     0     95   4334     29     61]\n",
            " [     0     26     15  10871     16]\n",
            " [     0     49     38     22   8053]]\n",
            "Validating..\n",
            "Validation loss:  0.029146196054560796\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9277390702033166\n",
            "\t f1 :  [0.         0.96817089 0.75335841 0.81860829 0.81612825]\n",
            "\t average f1 :  0.6712531665756298\n",
            "\t confusion matrix :  [[    0  1532   104   211   403]\n",
            " [    0 41064    30    48    22]\n",
            " [    0   249   701    35    22]\n",
            " [    0   576     5  2094    15]\n",
            " [    0   243    14    38  1680]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.0018922642274345788\n",
            "Training metrics:\n",
            "\t accuracy :  0.9495345370051806\n",
            "\t f1 :  [0.         0.9799473  0.92240117 0.96300207 0.89183977]\n",
            "\t average f1 :  0.7514380621057997\n",
            "\t confusion matrix :  [[     0   6673    587    771   1831]\n",
            " [     0 166984     21      9     16]\n",
            " [     0     62   4410     15     32]\n",
            " [     0     14      6  10945     15]\n",
            " [     0     39     19     11   8093]]\n",
            "Validating..\n",
            "Validation loss:  0.027809942407267436\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9289206698447623\n",
            "\t f1 :  [0.         0.9690707  0.75786164 0.8273889  0.81875916]\n",
            "\t average f1 :  0.6746160796840648\n",
            "\t confusion matrix :  [[    0  1505   113   236   396]\n",
            " [    0 41029    42    70    23]\n",
            " [    0   236   723    34    14]\n",
            " [    0   505     6  2169    10]\n",
            " [    0   238    17    44  1676]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.0016024524294253853\n",
            "Training metrics:\n",
            "\t accuracy :  0.949802139090737\n",
            "\t f1 :  [0.         0.97983542 0.92651824 0.9652958  0.89445241]\n",
            "\t average f1 :  0.7532203742745601\n",
            "\t confusion matrix :  [[     0   6732    578    732   1820]\n",
            " [     0 167059     21     14     12]\n",
            " [     0     60   4432      6     22]\n",
            " [     0     10      5  10973     12]\n",
            " [     0     27     11     10   8110]]\n",
            "Validating..\n",
            "Validation loss:  0.02830956982714789\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9293688628122071\n",
            "\t f1 :  [0.         0.96973137 0.75700935 0.82069219 0.82898696]\n",
            "\t average f1 :  0.6752839733905379\n",
            "\t confusion matrix :  [[    0  1497   123   304   326]\n",
            " [    0 41008    38   102    16]\n",
            " [    0   224   729    43    11]\n",
            " [    0   451     3  2229     7]\n",
            " [    0   232    26    64  1653]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.0011319066807455212\n",
            "Training metrics:\n",
            "\t accuracy :  0.9500727990745542\n",
            "\t f1 :  [0.         0.97965505 0.93081104 0.96890779 0.89848527]\n",
            "\t average f1 :  0.7555718279870713\n",
            "\t confusion matrix :  [[     0   6836    585    678   1784]\n",
            " [     0 166992     19      7      9]\n",
            " [     0     38   4453      0      9]\n",
            " [     0      5      2  10938      5]\n",
            " [     0     22      9      5   8156]]\n",
            "Validating..\n",
            "Validation loss:  0.0290209666958877\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9298374281872631\n",
            "\t f1 :  [0.         0.96927747 0.76874003 0.82867465 0.82989566]\n",
            "\t average f1 :  0.6793175625189356\n",
            "\t confusion matrix :  [[    0  1525   104   237   384]\n",
            " [    0 41030    36    74    24]\n",
            " [    0   234   723    34    16]\n",
            " [    0   497     2  2179    12]\n",
            " [    0   211     9    45  1710]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.0009499712420317034\n",
            "Training metrics:\n",
            "\t accuracy :  0.9502010238481708\n",
            "\t f1 :  [0.         0.97950134 0.93601846 0.9690236  0.90037677]\n",
            "\t average f1 :  0.7569840336958572\n",
            "\t confusion matrix :  [[     0   6886    544    675   1752]\n",
            " [     0 166765     15      4      8]\n",
            " [     0     39   4462      2      6]\n",
            " [     0      7      1  10902      5]\n",
            " [     0     21      3      3   8125]]\n",
            "Validating..\n",
            "Validation loss:  0.03024256495492799\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9296337041111519\n",
            "\t f1 :  [0.         0.96880643 0.76682316 0.83418267 0.82702444]\n",
            "\t average f1 :  0.6793673403106958\n",
            "\t confusion matrix :  [[    0  1537   121   180   412]\n",
            " [    0 41043    41    53    27]\n",
            " [    0   232   735    25    15]\n",
            " [    0   538     5  2128    19]\n",
            " [    0   215     8    26  1726]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.004543169607468708\n",
            "Training metrics:\n",
            "\t accuracy :  0.9467225784576723\n",
            "\t f1 :  [0.         0.9776491  0.92227001 0.9482444  0.88929582]\n",
            "\t average f1 :  0.7474918662523822\n",
            "\t confusion matrix :  [[     0   6874    559    708   1703]\n",
            " [     0 166806     39    118    102]\n",
            " [     0     85   4396      1     25]\n",
            " [     0    294     12  10654      9]\n",
            " [     0    115     20     21   8013]]\n",
            "Validating..\n",
            "Validation loss:  0.027554747249398912\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9270464083445381\n",
            "\t f1 :  [0.         0.9669041  0.77289571 0.8172     0.81503172]\n",
            "\t average f1 :  0.6744063075892479\n",
            "\t confusion matrix :  [[    0  1579    94   167   410]\n",
            " [    0 41062    36    51    15]\n",
            " [    0   239   730    20    18]\n",
            " [    0   626    11  2043    10]\n",
            " [    0   265    11    29  1670]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.0016368380187217284\n",
            "Training metrics:\n",
            "\t accuracy :  0.9493713202275222\n",
            "\t f1 :  [0.         0.97941128 0.93371212 0.96743567 0.88884014]\n",
            "\t average f1 :  0.7538798429895192\n",
            "\t confusion matrix :  [[     0   6797    515    662   1915]\n",
            " [     0 166829     32     21     26]\n",
            " [     0     54   4437      4     13]\n",
            " [     0     34      1  10903      7]\n",
            " [     0     50     11      5   8104]]\n",
            "Validating..\n",
            "Validation loss:  0.027164301435862268\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9297966833720409\n",
            "\t f1 :  [0.         0.96862953 0.77908218 0.83941316 0.81885619]\n",
            "\t average f1 :  0.6811962116790188\n",
            "\t confusion matrix :  [[    0  1561    95   153   441]\n",
            " [    0 41082    28    37    17]\n",
            " [    0   235   730    23    19]\n",
            " [    0   552     5  2117    16]\n",
            " [    0   231     9    24  1711]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.0006741323094401094\n",
            "Training metrics:\n",
            "\t accuracy :  0.950387751539785\n",
            "\t f1 :  [0.         0.97967697 0.93712856 0.97240613 0.89641894]\n",
            "\t average f1 :  0.7571261203928052\n",
            "\t confusion matrix :  [[     0   6852    559    601   1849]\n",
            " [     0 166983     11      7     11]\n",
            " [     0     26   4494      0      1]\n",
            " [     0      8      1  10942      3]\n",
            " [     0     13      5      1   8148]]\n",
            "Validating..\n",
            "Validation loss:  0.030168720920171057\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9287169457686509\n",
            "\t f1 :  [0.         0.96731595 0.78412698 0.83175499 0.82261209]\n",
            "\t average f1 :  0.6811620020856023\n",
            "\t confusion matrix :  [[    0  1612    95   143   400]\n",
            " [    0 41094    32    24    14]\n",
            " [    0   234   741    18    14]\n",
            " [    0   608     5  2064    13]\n",
            " [    0   253    10    24  1688]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.00047477594651152484\n",
            "Training metrics:\n",
            "\t accuracy :  0.9502992129128215\n",
            "\t f1 :  [0.         0.97930699 0.94206549 0.97498883 0.89839101]\n",
            "\t average f1 :  0.7589504635576416\n",
            "\t confusion matrix :  [[     0   7008    528    550   1823]\n",
            " [     0 166846      7      3      5]\n",
            " [     0     13   4488      1      1]\n",
            " [     0      4      0  10915      2]\n",
            " [     0     11      2      0   8152]]\n",
            "Validating..\n",
            "Validation loss:  0.031716157283101766\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9287984353990955\n",
            "\t f1 :  [0.         0.96741395 0.78552422 0.83201621 0.82119526]\n",
            "\t average f1 :  0.6812299257415384\n",
            "\t confusion matrix :  [[    0  1612    89   133   416]\n",
            " [    0 41103    30    18    13]\n",
            " [    0   235   738    18    16]\n",
            " [    0   616     5  2053    16]\n",
            " [    0   245    10    23  1697]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": True,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vA-Yjqg7n0V"
      },
      "source": [
        "We were using bidirectional LSTMs. Please re-run the experiment with a regular (unidirectional) LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7wNrdvJ98ARB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec013468-a5b2-42f9-8287-ab24cbec9429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': False, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (words_vocab): Vocab()\n",
            "  (tags_vocab): Vocab()\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True)\n",
            "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.21564551649822128\n",
            "Training metrics:\n",
            "\t accuracy :  0.8086653242126407\n",
            "\t f1 :  [0.01954658 0.89919945 0.01908471 0.19006205 0.37143826]\n",
            "\t average f1 :  0.2998662124952528\n",
            "\t confusion matrix :  [[   122   8682     27    551    510]\n",
            " [  2111 157533    484   6179    312]\n",
            " [    56   3755     49    337    321]\n",
            " [   179   8495     24   1945    296]\n",
            " [   123   5301     33    516   2190]]\n",
            "Validating..\n",
            "Validation loss:  0.06024751333253724\n",
            "Validation metrics:\n",
            "\t accuracy :  0.8834494560567168\n",
            "\t f1 :  [0.         0.94480168 0.11655874 0.55388128 0.59462395]\n",
            "\t average f1 :  0.4419731292315312\n",
            "\t confusion matrix :  [[    0  1837     5   121   287]\n",
            " [    0 40994     2   117    51]\n",
            " [    0   670    63    85   189]\n",
            " [    0  1391     0  1213    86]\n",
            " [    0   722     4   154  1095]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.045227809705668025\n",
            "Training metrics:\n",
            "\t accuracy :  0.8995425361556075\n",
            "\t f1 :  [0.         0.95769176 0.44201706 0.69104097 0.67443061]\n",
            "\t average f1 :  0.5530360812911018\n",
            "\t confusion matrix :  [[     0   7271    353    793   1490]\n",
            " [     0 165911     81    600    287]\n",
            " [     0   1558   1477    417   1076]\n",
            " [     0   3325     64   7050    527]\n",
            " [     0   1537    180    578   5878]]\n",
            "Validating..\n",
            "Validation loss:  0.035377402124660354\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9125616265330236\n",
            "\t f1 :  [0.         0.96250368 0.59098316 0.74160752 0.72966692]\n",
            "\t average f1 :  0.6049522565078119\n",
            "\t confusion matrix :  [[    0  1559   127   230   334]\n",
            " [    0 40904    56   177    27]\n",
            " [    0   316   544    70    77]\n",
            " [    0   696    14  1933    47]\n",
            " [    0   356    93   113  1413]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.02466827027361702\n",
            "Training metrics:\n",
            "\t accuracy :  0.9252379360080651\n",
            "\t f1 :  [0.         0.97186904 0.69597819 0.8321403  0.77130954]\n",
            "\t average f1 :  0.6542594133088223\n",
            "\t confusion matrix :  [[     0   6321    691   1079   1759]\n",
            " [     0 166176    133    407    161]\n",
            " [     0    696   3063    222    526]\n",
            " [     0   1268    101   9300    300]\n",
            " [     0    634    307    375   6850]]\n",
            "Validating..\n",
            "Validation loss:  0.028153606025235995\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9181232938108626\n",
            "\t f1 :  [0.         0.96852092 0.64717742 0.74873865 0.76727717]\n",
            "\t average f1 :  0.6263428317858771\n",
            "\t confusion matrix :  [[    0  1313   162   378   397]\n",
            " [    0 40628    75   408    53]\n",
            " [    0   221   642    85    59]\n",
            " [    0   400    24  2226    40]\n",
            " [    0   171    74   159  1571]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.013814800108472506\n",
            "Training metrics:\n",
            "\t accuracy :  0.9369584314978238\n",
            "\t f1 :  [0.         0.97790351 0.78150091 0.89370062 0.81512976]\n",
            "\t average f1 :  0.693646958871896\n",
            "\t confusion matrix :  [[     0   6011    795   1139   1969]\n",
            " [     0 166447     83    166     97]\n",
            " [     0    395   3650    139    324]\n",
            " [     0    467     87  10236    171]\n",
            " [     0    303    218    266   7381]]\n",
            "Validating..\n",
            "Validation loss:  0.026259355513112887\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9186937212239743\n",
            "\t f1 :  [0.         0.97034737 0.64992757 0.74551857 0.77545195]\n",
            "\t average f1 :  0.6282490907922839\n",
            "\t confusion matrix :  [[    0  1191   184   435   440]\n",
            " [    0 40463   118   514    69]\n",
            " [    0   174   673   113    47]\n",
            " [    0   290    28  2329    43]\n",
            " [    0   117    61   167  1630]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.009179450933718018\n",
            "Training metrics:\n",
            "\t accuracy :  0.9423339819388316\n",
            "\t f1 :  [0.         0.98036114 0.81853282 0.91871964 0.83730853]\n",
            "\t average f1 :  0.7109844260451156\n",
            "\t confusion matrix :  [[     0   5836    815   1164   2035]\n",
            " [     0 166706     71     83     58]\n",
            " [     0    268   3922     91    252]\n",
            " [     0    197     59  10591    108]\n",
            " [     0    166    183    172   7653]]\n",
            "Validating..\n",
            "Validation loss:  0.025643821007439067\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9210161756916433\n",
            "\t f1 :  [0.         0.9707954  0.66601082 0.7636603  0.78052158]\n",
            "\t average f1 :  0.6361976205911495\n",
            "\t confusion matrix :  [[    0  1194   163   399   494]\n",
            " [    0 40521   117   446    80]\n",
            " [    0   189   677    90    51]\n",
            " [    0   305    23  2320    42]\n",
            " [    0   107    46   131  1691]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.0066043036144778685\n",
            "Training metrics:\n",
            "\t accuracy :  0.9446793006553237\n",
            "\t f1 :  [0.         0.98105843 0.8484597  0.93136915 0.84849145]\n",
            "\t average f1 :  0.7218757471462138\n",
            "\t confusion matrix :  [[     0   5847    790   1144   2125]\n",
            " [     0 166647     59    113     49]\n",
            " [     0    183   4090     52    207]\n",
            " [     0     85     29  10748     64]\n",
            " [     0     99    141     97   7790]]\n",
            "Validating..\n",
            "Validation loss:  0.06951847938554627\n",
            "Validation metrics:\n",
            "\t accuracy :  0.8998084993684554\n",
            "\t f1 :  [0.         0.95490859 0.62875888 0.55461821 0.75031431]\n",
            "\t average f1 :  0.5777199988383807\n",
            "\t confusion matrix :  [[    0  1461   131   255   403]\n",
            " [    0 40819    47   269    29]\n",
            " [    0   358   575    44    30]\n",
            " [    0  1337    23  1282    48]\n",
            " [    0   354    46    83  1492]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.00722722256452673\n",
            "Training metrics:\n",
            "\t accuracy :  0.9448106423777565\n",
            "\t f1 :  [0.         0.98042606 0.85880883 0.9298919  0.85564762]\n",
            "\t average f1 :  0.7249548838362745\n",
            "\t confusion matrix :  [[     0   5899    767   1075   2114]\n",
            " [     0 166694     61     33     40]\n",
            " [     0    202   4124     32    164]\n",
            " [     0    311     24  10538     47]\n",
            " [     0    110    106     67   7848]]\n",
            "Validating..\n",
            "Validation loss:  0.02784362355513232\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9176547284358065\n",
            "\t f1 :  [0.         0.97026453 0.64175824 0.74662577 0.78817734]\n",
            "\t average f1 :  0.6293651756725798\n",
            "\t confusion matrix :  [[    0  1106   211   458   475]\n",
            " [    0 40200   227   673    64]\n",
            " [    0   125   730   117    35]\n",
            " [    0   195    27  2434    34]\n",
            " [    0    74    73   148  1680]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.0033277357600767304\n",
            "Training metrics:\n",
            "\t accuracy :  0.9478876259918507\n",
            "\t f1 :  [0.         0.98188834 0.87893487 0.94713004 0.86570069]\n",
            "\t average f1 :  0.7347307898824136\n",
            "\t confusion matrix :  [[     0   5901    777   1080   2111]\n",
            " [     0 166922     46     21     31]\n",
            " [     0     96   4258     19    146]\n",
            " [     0     21      4  10874     33]\n",
            " [     0     42     85     36   8006]]\n",
            "Validating..\n",
            "Validation loss:  0.02879008331469127\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9182862730717516\n",
            "\t f1 :  [0.         0.97003605 0.64544651 0.7521341  0.79271709]\n",
            "\t average f1 :  0.6320667493143944\n",
            "\t confusion matrix :  [[    0  1122   206   447   475]\n",
            " [    0 40224   242   628    70]\n",
            " [    0   129   730   109    39]\n",
            " [    0   214    26  2423    27]\n",
            " [    0    80    51   146  1698]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.002556973749219819\n",
            "Training metrics:\n",
            "\t accuracy :  0.9486707749990029\n",
            "\t f1 :  [0.         0.98165642 0.89531057 0.95148683 0.87344029]\n",
            "\t average f1 :  0.740378822375021\n",
            "\t confusion matrix :  [[     0   6049    731   1020   2079]\n",
            " [     0 166940     40     15     24]\n",
            " [     0     58   4353     11    112]\n",
            " [     0     20      8  10895     19]\n",
            " [     0     33     58     18   8085]]\n",
            "Validating..\n",
            "Validation loss:  0.030012564467532293\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9171861630607505\n",
            "\t f1 :  [0.         0.96945856 0.64137931 0.74650699 0.79468943]\n",
            "\t average f1 :  0.6304068557420432\n",
            "\t confusion matrix :  [[    0  1130   214   451   455]\n",
            " [    0 40170   267   672    55]\n",
            " [    0   118   744   113    32]\n",
            " [    0   210    24  2431    25]\n",
            " [    0    79    64   156  1676]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.002128549894593932\n",
            "Training metrics:\n",
            "\t accuracy :  0.948931815912948\n",
            "\t f1 :  [0.         0.98142944 0.89899928 0.95368347 0.87814692]\n",
            "\t average f1 :  0.7424518227440314\n",
            "\t confusion matrix :  [[     0   6137    721   1001   2012]\n",
            " [     0 166764     34     10     29]\n",
            " [     0     61   4357      9     96]\n",
            " [     0     15      2  10913     12]\n",
            " [     0     25     56     11   8075]]\n",
            "Validating..\n",
            "Validation loss:  0.03110320759671075\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9175121215825286\n",
            "\t f1 :  [0.         0.96907615 0.68115257 0.73993339 0.79342288]\n",
            "\t average f1 :  0.6367169989124537\n",
            "\t confusion matrix :  [[    0  1160   158   456   476]\n",
            " [    0 40159   182   743    80]\n",
            " [    0   127   721   116    43]\n",
            " [    0   196    19  2444    31]\n",
            " [    0    75    30   157  1713]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.0019879174824045213\n",
            "Training metrics:\n",
            "\t accuracy :  0.9492933047034968\n",
            "\t f1 :  [0.         0.98136905 0.90295272 0.95565468 0.8796226 ]\n",
            "\t average f1 :  0.7439198097343711\n",
            "\t confusion matrix :  [[     0   6178    694    956   2008]\n",
            " [     0 167135     36     14     25]\n",
            " [     0     58   4373      5     90]\n",
            " [     0     12      5  10926     13]\n",
            " [     0     23     52      9   8111]]\n",
            "Validating..\n",
            "Validation loss:  0.03189799216176782\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9191011693761969\n",
            "\t f1 :  [0.         0.96968096 0.67347887 0.74949337 0.78958998]\n",
            "\t average f1 :  0.6364486368113019\n",
            "\t confusion matrix :  [[    0  1212   183   442   413]\n",
            " [    0 40378   156   600    30]\n",
            " [    0   149   725   107    26]\n",
            " [    0   251    14  2404    21]\n",
            " [    0   127    68   172  1608]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.0017976548923696908\n",
            "Training metrics:\n",
            "\t accuracy :  0.9493001849885068\n",
            "\t f1 :  [0.         0.98108307 0.90418298 0.95827313 0.88202554]\n",
            "\t average f1 :  0.7451129440901829\n",
            "\t confusion matrix :  [[     0   6289    707    905   1951]\n",
            " [     0 166972     34      8     24]\n",
            " [     0     44   4388      6     92]\n",
            " [     0     13      0  10943     11]\n",
            " [     0     27     47     10   8082]]\n",
            "Validating..\n",
            "Validation loss:  0.03224239684641361\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9178380801043067\n",
            "\t f1 :  [0.         0.96905728 0.68290353 0.73621868 0.79682234]\n",
            "\t average f1 :  0.6370003657136522\n",
            "\t confusion matrix :  [[    0  1202   163   452   433]\n",
            " [    0 40259   149   713    43]\n",
            " [    0   146   715   117    29]\n",
            " [    0   234    13  2424    19]\n",
            " [    0    84    47   189  1655]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.0015070589660252961\n",
            "Training metrics:\n",
            "\t accuracy :  0.9495030098699796\n",
            "\t f1 :  [0.         0.98081646 0.91341678 0.95982437 0.88553789]\n",
            "\t average f1 :  0.747919099669721\n",
            "\t confusion matrix :  [[     0   6412    655    877   1929]\n",
            " [     0 166933     21      7     21]\n",
            " [     0     41   4415      4     79]\n",
            " [     0     12      1  10930      8]\n",
            " [     0     16     36      6   8104]]\n",
            "Validating..\n",
            "Validation loss:  0.035980637052229474\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9136821089516359\n",
            "\t f1 :  [0.         0.96863152 0.55486111 0.7667304  0.77687627]\n",
            "\t average f1 :  0.6134198606829019\n",
            "\t confusion matrix :  [[    0  1161   316   398   375]\n",
            " [    0 40112   467   560    25]\n",
            " [    0   107   799    85    16]\n",
            " [    0   204    59  2406    21]\n",
            " [    0    74   232   137  1532]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.009079616055478927\n",
            "Training metrics:\n",
            "\t accuracy :  0.9443560177030891\n",
            "\t f1 :  [0.         0.97849974 0.878401   0.9378105  0.86023031]\n",
            "\t average f1 :  0.7309883117598458\n",
            "\t confusion matrix :  [[     0   6335    670    846   2006]\n",
            " [     0 166616     87    105    101]\n",
            " [     0    163   4197     13    125]\n",
            " [     0    363      9  10571     36]\n",
            " [     0    168     95     30   7881]]\n",
            "Validating..\n",
            "Validation loss:  0.03044264736984457\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9163508943486941\n",
            "\t f1 :  [0.         0.96897687 0.66313933 0.73965721 0.79424257]\n",
            "\t average f1 :  0.6332031946400936\n",
            "\t confusion matrix :  [[    0  1125   204   445   476]\n",
            " [    0 40042   224   847    51]\n",
            " [    0   108   752   117    30]\n",
            " [    0   147    17  2503    23]\n",
            " [    0    62    64   166  1683]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.0016178222075622114\n",
            "Training metrics:\n",
            "\t accuracy :  0.9491669992019154\n",
            "\t f1 :  [0.         0.98079969 0.91063653 0.9601513  0.88037964]\n",
            "\t average f1 :  0.7463934326965598\n",
            "\t confusion matrix :  [[     0   6376    670    832   2008]\n",
            " [     0 166912     24     23     18]\n",
            " [     0     41   4392      4     77]\n",
            " [     0     27      1  10915     11]\n",
            " [     0     26     45      8   8070]]\n",
            "Validating..\n",
            "Validation loss:  0.029749100229569843\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9193456382675305\n",
            "\t f1 :  [0.         0.96961735 0.67546655 0.75769582 0.79847182]\n",
            "\t average f1 :  0.640250309723495\n",
            "\t confusion matrix :  [[    0  1208   191   400   451]\n",
            " [    0 40227   181   712    44]\n",
            " [    0   127   742   111    27]\n",
            " [    0   169    16  2486    19]\n",
            " [    0    80    60   163  1672]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "## TODO: Re-run with unidirectional LSTMs\n",
        "## Keep other hyperparameters fixed\n",
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": False,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})\n",
        "## END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8UChDyKaPBs"
      },
      "source": [
        "### Questions **(2 points)**\n",
        "\n",
        "(a) How does the final performance of LSTMs compare to FFNNs? Is it better? What is a possible explanation?\n",
        "\n",
        "**TODO: LSTMs often perform better than FFNNs, especially in tasks involving sequential data like natural language processing (NLP), time series analysis, and speech recognition. LSTMs are designed to capture long-range dependencies in sequences, making them particularly effective for tasks where context and order matter.\n",
        "\n",
        "(b) How does bidirectional LSTMs compare to unidirectional LSTMs? Why?\n",
        "\n",
        "**TODO:  the choice between bidirectional and unidirectional LSTMs depends on the requirements of the specific task, including the importance of future context, computational constraints, and desired model complexity.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMEWxkN_bpIT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}